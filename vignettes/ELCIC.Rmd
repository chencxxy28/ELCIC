---
title: "ELCIC"
author: "Chixiang Chen"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{ELCIC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**ELCIC** is a robust and consistent model selection criterion based upon the empirical likelihood function which is data-driven. In particular, this framework adopts plug-in estimators that can be achieved by solving external estimating equations, not limited to the empirical likelihood, which avoids potential computational convergence issues and allows versatile applications, such as generalized linear models, generalized estimating equations, penalized regressions, and so on. The formulation of our proposed criterion is initially derived from the asymptotic expansion of the marginal likelihood under the variable selection framework, but more importantly, the consistent model selection property is established under a general context. 

## Installation
```{r, eval=FALSE}
if (!require("devtools")) {
  install.packages("devtools")
}
devtools::install_github("chencxxy28/ELCIC")
```

```{r}
library(ELCIC)
library(MASS)
library(mvtnorm)
```

We provide some basic tutorial for illustrating the usage of **ELCIC** package. More technical details are referred to the [Chen et al. (2021)](https://arxiv.org/pdf/2006.13281.pdf). In the following, we list three applications of ELCIC, which is the variable selection in generalized linear model, model selection in longitudinal data, and model selection in longitudinal data with missingness.

## Variable Selection in Generalized Linear Models (GLMs)
First, let us generate a pseudo data from negative binomial distribution with overdispersion paramter (ov) equal to 8. We provide a function `glm.generator` to generate responses and covariates. Different outcome distributions are considered in this function including Gaussian, Poisson, Binomial, and Negative Binomial distributions. 

We first test how ELCIC works when the distribution is correctly specified. The outcomes are generated from Poisson distribution. Then, we consider seven candidate models with different covariate combinations and compare ELCIC with most commonly used criteria including [AIC](https://ieeexplore.ieee.org/document/1100705), [BIC](https://projecteuclid.org/journals/annals-of-statistics/volume-6/issue-2/Estimating-the-Dimension-of-a-Model/10.1214/aos/1176344136.full), and [GIC](https://www.jstor.org/stable/2337290?seq=1). The function `ELCIC.glm` can produce the values of ELCIC, AIC, BIC, and GIC given a candidate model. The selection rates from four criteria are present based on 20 Monte Carlo runs (may take 1-2 minutes to run).

```{r,message=FALSE}
set.seed(28582)
inter.total=20
count.matrix<-matrix(0,nrow=4,ncol=7)
rownames(count.matrix)<-c("ELCIC","AIC","BIC","GIC")
colnames(count.matrix)<-rep(1:7)
samplesize<-100
ov=2
candidate.sets<-list(c(1,2),c(1,3),c(1,4),c(1,2,3),c(1,2,4),c(1,3,4),c(1,2,3,4))
for(iter in 1:inter.total)
{
    #generate data
simulated.data<-glm.generator(beta=c(0.5,0.5,0.5,0),samplesize=samplesize,rho=0.5,dist="poisson")
y<-simulated.data[["y"]]
x<-simulated.data[["x"]]

criterion.all<-rep()
for (i in 1:length(candidate.sets))
{
    criterion<-ELCIC.glm(x=x,y=y,index.var=candidate.sets[[i]],name.var=NULL,dist="poisson")
    criterion.all<-cbind(criterion.all,criterion)
}
#print(iter)
index.used<-cbind(1:4,apply(criterion.all,1,which.min))
count.matrix[index.used]<-count.matrix[index.used]+1
}
count.matrix/inter.total
```

Based on the results, we find ELCIC and BIC has the best performance due to the highest selection rate among others. It shows that ELCIC is as powerfull as BIC when the distribution is correctly specified. Next, we consider the situation where the AIC, BIC, and GIC is based on misspecified distribution. Accrodingly, we generate outcomes from Negative Binomial distribution with the dispersion parameter equal to 2, while AIC, BIC, and GIC consider Poisson distribution as the input (may take 1-2 minutes to run). 

```{r,message=FALSE}
set.seed(28582)
inter.total=20
count.matrix<-matrix(0,nrow=4,ncol=7)
rownames(count.matrix)<-c("ELCIC","AIC","BIC","GIC")
colnames(count.matrix)<-rep(1:7)
samplesize<-100
ov=2
candidate.sets<-list(c(1,2),c(1,3),c(1,4),c(1,2,3),c(1,2,4),c(1,3,4),c(1,2,3,4))
for(iter in 1:inter.total)
{
    #generate data
simulated.data<-glm.generator(beta=c(0.5,0.5,0.5,0),samplesize=samplesize,rho=0.5,dist="NB",ov=2)
y<-simulated.data[["y"]]
x<-simulated.data[["x"]]

criterion.all<-rep()
for (i in 1:length(candidate.sets))
{
    criterion<-ELCIC.glm(x=x,y=y,index.var=candidate.sets[[i]],name.var=NULL,dist="poisson")
    criterion.all<-cbind(criterion.all,criterion)
}
#print(iter)
index.used<-cbind(1:4,apply(criterion.all,1,which.min))
count.matrix[index.used]<-count.matrix[index.used]+1
}
count.matrix/inter.total
```

The above results show that ELCIC has the highest selection rate. The better performance is attributed to its distribution free property, which is highly valuable in real applications where the underlying distribution is hard to be correctly specified.

## Model Selection in longitudinal data analysis (no missing data)
coming soon

## Model Selection in longitudinal data analysis (dropout missing)
coming soon

## Mondel Selection in high-dimensional longitudal data analysis
coming soon


